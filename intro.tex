\section{Introduction}\label{sec:intro}

\todo{Shouldn't we talk about motivation?}
We give fast and scalable practical algorithms for the problem of partitioining large graphs into components of roughly the same size/weight according to multiple user-specified weight functions. This problem, referred to as \textit{multi-dimensional balanced graph partitioning} (see Section~\ref{sec:mdbgp-definitions} for formal definitions) arises in critical infrastructure applications which involve storage and processing of large graphs, including social networks. High-quality partitions help optimize load balancing in query processing, \todo{elaborate} etc.  
\todo{Talk about why multi-dimensional is particularly important vs. one-dimensional}
While a large body of work exists offering practical solutions for the one-dimensional version of the problem~\cite{KK95, DGRW12, UB13, TGRV14, ABM16, DKKOPS16, MLLS17, KKPPSAP17} (see also a recent survey~\cite{BMSSS16}), as well as on theoretical foundations of graph partitioning~\cite{KNS09,AFKRS14,MM14}\todo{cite a bunch of theory papers}, literature on principled and scalable approaches for the multi-dimensional case is quite sparse~\cite{}. In particular, if the weight functions are unrelated to each other, one can easily construct examples when no feasible solution exists that satisfies all balance constraints even for two weight functions. 
\todo{Say that in general no feasible solution might exist at all.}

\paragraph {Our contributions}
Let $G(V,E)$ be an $n$-vertex graph whose adjacency matrix is $A$. We introduce a family of algorithms for the multi-dimensional balanced graph partitioning problem by using the \textit{projected gradient descent method} on a standard relaxation which involves maximizing an $n$-dimensional non-convex quadratic function $f(\vx) = \vx^T A \vx$ subject to a constraint $\vx \in K$ for some convex body $K$ defined by the weight functions \footnote{While second-order methods could potentially give better performance in terms of partition quality, due to the large scale of our instances such methods are infeasible.}. See Section~\ref{sec:algorithm} for the exact description of the relaxation\todo{Maybe need a citation here to justify this is standard}.

While applying projected gradient descent to solve \todo{discuss how constrained is different from unconstrained} non-convex optimization problems subject to convex constraints is a well-studied approach in non-linear optimization~\cite{B99} (Section 2.3) and machine learning~\cite{JK17} (Section 6.6), one has to overcome several technical challenges to make it applicable to the multi-dimensional graph partitoning problem: 1) projection step is computationally expensive, 2) abundance of saddle points slows down convergence.

We show how to address the first challenge by designing ultra-efficient projection step algorithms tailored to the standard non-convex relaxation of the multi-dimensional balanced graph partitioning problem. Computationally the problem of finding the closest point in the 
 For balance according to one weight function our projection algorithm runs in time $O(n)$ where $n$ is the number of vertices in the graph. For two weight functions we show how to implement projection in time $O(n \log^2 n)$. For $k$ weight functions the time is \todo{TBD}.

In order to address the second challenge we use small perturbations to each intermediate point, where the perturbation vectors are sampled from a scaled $n$-dimensional Gaussian distribution. \todo{cite some literature on randomized PGD} We refer to this algorithm as \algname (Algorithm~\ref{alg:mdgp-rpgd}).
We show how the magnitude of Gaussian noise affects convergence properties of \algname by helping it escape from saddle points~\cite{} \todo{cite literature on escaping from saddle points using SGD, etc.}.

\input{expintro}

\paragraph{Previous work}

While one-dimensional balanced graph partitioning has been studied extensively and a number of tools exist, to the best of our knowledge none of the practical algorithms for this problem have been previously based on running gradient descent on a continuous relaxation\todo{We will have to triple check this by looking at various NIPS papers}.
Existing approaches are inherently discrete and are based on combinations of various discrete algorithms ideas with heuristics: combinatorial heuristics and greedy methods (e.g. METIS~\cite{KK95}, FENNEL~\cite{TGRV14}), branch-and-bound (e.g.~\cite{DGRW12}), label propagation and local search (e.g. balanced label propagration~\cite{UB13}, Social Hash Partitioner~\cite{KKPPSAP17}, Spinner~\cite{MLLS17}), as well as hybrid approaches (e.g. linear embedding method combined with various optimizations~\cite{ABM16}). See~\cite{BMSSS16} for a recent survey of this topic. Due to the combinatorial nature of these algorithms their generalizations to the multi-dimensional case appear to be non-straightforward without substantial losses in performance, while a continuous relaxation handles multiple balance constrains uniformly.

Vast literature exists on optimization of non-convex functions and the interest in this topic lately has been particularly high~\cite{}\todo{cite all sorts of classic and recent gradient descent papers}. However, in the constrained case when the optimization has to be performed over a convex body fairly little is known for the general case (see e.g. classic texts~\cite{B99,WN99,BV04}). We refer to the convex constrained non-convex optimization problem as CNOPT, as in Section 6.6 of a recent survey on non-convex optimization for machine learning~\cite{JK17}.

\begin{itemize}
%\item Discuss all papers on balanced graph partitioning again~\cite{KK95, DGRW12, UB13, TGRV14, ABM16,  DKKOPS16, MLLS17, KKPPSAP17}, survey~\cite{BMSSS16}.
\item Discuss all the standard non-convex optimization subject to convex constraints (CNOPT) literature again~\cite{B99} (Section 2.3),~\cite{JK17} (Section 6.6).
\item Discuss recent papers which use first-order methods for CNOPT and show how to escape saddle points in general and specific situations. Cite noisy SGD~\cite{GHJY15}, some polynomial time algortihm which converges to third-order local optimum~\cite{AG16}, matrix completion~\cite{GLM16}, second-order methods~\cite{SQW15}.
\item Discuss papers which use PGD for constrained non-convex optimization problems arising from graph partitioning~\cite{LRSST10}.\todo{Find more papers to cite here}.
\item Discuss why we can't use off the shelf QP solvers, like OSQP from Steven Boyd and others (they wouldn't scale to billions of vertices)~\cite{SBGBB17}.\todo{seems like they are only looking at the convex case, their number of non-zeros is at most $10^8$ it seems}
\end{itemize}
\todo{How about SDP approaches?}

\paragraph{Our techniques}



\section{Preliminaries}\label{sec:mdbgp-definitions}

We study multi-dimensional graph partitioning problems. The basic one-dimensional unweighted graph partitioning problem is as follows: 

\begin{definition}[\textsc{$(1 \pm \epsilon)$-Balanced $k$-Partition}]
	Given an input graph $G(V, E)$, an integer $k$ and a parameter $\epsilon > 0$ the goal is to find a partition of the vertex set $V$ into $k$ sets $V_1, \dots, V_k$ such that $|V_i| = \frac{(1\pm \epsilon) |V|}{k}$ for all $i \in [k]$. Among all such partitions the goal is to find one that maximizes the number of edges whose both endpoints are contained within some part of the partition. 
\end{definition}

The more general weighted $d$-dimensional version is defined by a collection of $d$ weight functions $w_1, \dots, w_d$ where each $w_i \colon V \to \mathbb R^+$ is a real-valued weight function. For a set $S \subseteq V$ we use notation $w_i(S) \equiv \sum_{v \in S} w_i(v)$. For example, the unweighted case above corresponds to $d = 1$ and $w_1(v) = 1$ for all $v \in V$.

\begin{definition}[\textsc{Multi-dimensional Weighted $(1 \pm \epsilon)$-Balanced $k$-Partition}]
	Given an input graph $G(V, E)$, an integer $k$ and a parameter $\epsilon > 0$ the goal is to find a partition of the vertex set $V$ into $k$ sets $V_1, \dots, V_k$ such that for each $j \in [d]$ it holds that  $w_j(V_i) = \frac{(1\pm \epsilon) w_j(V)}{k}$ for all $i \in [k]$. Among all such partitions the goal is to find one that maximizes the number of edges whose both endpoints are contained within some part of the partition. 
\end{definition}

